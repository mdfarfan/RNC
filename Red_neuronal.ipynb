{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Red neuronal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdfarfan/RNC/blob/main/Red_neuronal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpVgSST_Y455",
        "outputId": "f087c6f2-a516-4887-e0f3-800cae5fd9b2"
      },
      "source": [
        "pip install --upgrade keras"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsvzwGy-bWrT"
      },
      "source": [
        "# Clonamos el repositorio para obtener el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWYwSH-3Z97s",
        "outputId": "141d88e3-84df-4af2-d415-17bbc7010d7a"
      },
      "source": [
        "!git clone https://github.com/joanby/deeplearning-az.git"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning-az'...\n",
            "remote: Enumerating objects: 10196, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 10196 (delta 50), reused 67 (delta 30), pack-reused 10096\u001b[K\n",
            "Receiving objects: 100% (10196/10196), 281.16 MiB | 40.03 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "Checking out files: 100% (10119/10119), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj7yYesKbctW"
      },
      "source": [
        "# Parte 1 - Construir el modelo de CNN\n",
        "\n",
        "# Importar las librerías y paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBacLjT1Y81H"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import logging, os\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtczsRskbj2F"
      },
      "source": [
        "#RED 1\n",
        "Inicializar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQL9sTJ9Y_Vc"
      },
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbAgrrqbpsY"
      },
      "source": [
        "# Paso 1 - Convolución y Maxpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kYdJsjsZCR8"
      },
      "source": [
        "#Capa 1\n",
        "classifier.add(Conv2D(filters = 64,kernel_size = (3, 3),input_shape = (128,128, 3), activation = \"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "#Capa 2\n",
        "classifier.add(Conv2D(filters = 64,kernel_size = (3, 3), activation = \"relu\"))\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfacNAkPb_T-"
      },
      "source": [
        "# Paso 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWl9_DHxZJZD"
      },
      "source": [
        "classifier.add(Flatten())"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksz0Q4WGcB77"
      },
      "source": [
        "# Paso 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQnTQSjyZKXs"
      },
      "source": [
        "#Capa 1\n",
        "classifier.add(Dense(units = 256, activation = \"relu\"))\n",
        "classifier.add(Dropout(0.5))\n",
        "\n",
        "#Capa 2\n",
        "classifier.add(Dense(units = 256, activation = \"relu\"))\n",
        "classifier.add(Dropout(0.5))\n",
        "\n",
        "#Salida\n",
        "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7INHvHmcFdL"
      },
      "source": [
        "# Compilar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc7pBop7ZLeN"
      },
      "source": [
        "classifier.compile(optimizer = 'adam', loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "batch = 32"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1JYggq51iL5"
      },
      "source": [
        "#Filtros capa 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GY76yZeGTAZ",
        "outputId": "4bee68b0-921a-42eb-ecc3-3284ec504e42"
      },
      "source": [
        "#Mostrar los filtros\n",
        "layer = classifier.layers #Obtenemos las capas \n",
        "filters,biases = classifier.layers[2].get_weights()\n",
        "print(layer[2].name,filters.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_7 (3, 3, 64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUSdN8u1qV-"
      },
      "source": [
        "#Mostrar filtros\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "gC6sTnb41uLt",
        "outputId": "c518ed06-ab84-4618-94f7-3e6271631278"
      },
      "source": [
        "#Creamos la imagen para mostrar\n",
        "fig1 = plt.figure(figsize=(8,12))\n",
        "columnas = 8\n",
        "filas = 8\n",
        "nfiltros = columnas*filas\n",
        "\n",
        "#For para graficar todos los filtros\n",
        "for i in range(1,nfiltros+1):\n",
        "    f = filters[:,:,:,i-1]\n",
        "    fig1 = plt.subplot(filas,columnas,i)\n",
        "    fig1.set_xticks([])\n",
        "    fig1.set_yticks([])\n",
        "    plt.imshow(f[:,:,0], cmap = 'gray')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x864 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hits0FnccMjr"
      },
      "source": [
        "# Parte 2 - Ajustar la CNN a las imágenes para entrenar "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYgCwVDFZMrU",
        "outputId": "78a5d721-365f-4cd4-f2b1-68344d567706"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_dataset = train_datagen.flow_from_directory('/content/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
        "                                                    target_size=(128, 128),\n",
        "                                                    batch_size=batch,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "testing_dataset = test_datagen.flow_from_directory('/content/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
        "                                                target_size=(128, 128),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-7yFwgGtUTU"
      },
      "source": [
        "#Entrenar la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYvCeprStZpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eb6dff-580b-4252-9da1-6a6760983ae7"
      },
      "source": [
        "history = classifier.fit(training_dataset,\n",
        "                        steps_per_epoch=8000/32,\n",
        "                        epochs=25,\n",
        "                        validation_data=testing_dataset,\n",
        "                        validation_steps=2000/32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 79s 187ms/step - loss: 0.7232 - accuracy: 0.5321 - val_loss: 0.6765 - val_accuracy: 0.5690\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 47s 186ms/step - loss: 0.6745 - accuracy: 0.5837 - val_loss: 0.6831 - val_accuracy: 0.6050\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 46s 186ms/step - loss: 0.6378 - accuracy: 0.6460 - val_loss: 0.6345 - val_accuracy: 0.6655\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 46s 184ms/step - loss: 0.6186 - accuracy: 0.6706 - val_loss: 0.5898 - val_accuracy: 0.6910\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 46s 184ms/step - loss: 0.5849 - accuracy: 0.6906 - val_loss: 0.5788 - val_accuracy: 0.6955\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 46s 184ms/step - loss: 0.5612 - accuracy: 0.7103 - val_loss: 0.5677 - val_accuracy: 0.7150\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 46s 184ms/step - loss: 0.5509 - accuracy: 0.7241 - val_loss: 0.5435 - val_accuracy: 0.7340\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 47s 187ms/step - loss: 0.5229 - accuracy: 0.7427 - val_loss: 0.5282 - val_accuracy: 0.7380\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 46s 186ms/step - loss: 0.5085 - accuracy: 0.7538 - val_loss: 0.5023 - val_accuracy: 0.7720\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 46s 185ms/step - loss: 0.4871 - accuracy: 0.7703 - val_loss: 0.4946 - val_accuracy: 0.7600\n",
            "Epoch 11/25\n",
            " 78/250 [========>.....................] - ETA: 29s - loss: 0.4716 - accuracy: 0.7746"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyuyDy1ymYEw"
      },
      "source": [
        "#Guardar el modelo \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o64rckrnmTbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f271a9af-cc0d-4851-ab5c-5ed5f1edaae0"
      },
      "source": [
        "#Guardamos modelo\n",
        "!mkdir -p saved_model\n",
        "classifier.save(\"model.h5\")\n",
        "classifier.save('/content/RNC/saved_model/RED1') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/RNC/saved_model/RED1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkODjyzDZF-K"
      },
      "source": [
        "RED1 = tf.keras.models.load_model('/content/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/saved_model/RED1')\n",
        "\n",
        "# Check its architecture\n",
        "RED1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn4AsYfbnZjp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17c3756c-4ddf-4698-f5f4-c4ba3443c016"
      },
      "source": [
        "\"Mostrar los salidas red 1________________________________________________________________\"\n",
        "#Mostrar las imagenes de salida de las convoluciones\n",
        "conv_layer = [0,2] #indices de las capas convolucionales\n",
        "outputs = [classifier.layers[i].output for i in conv_layer] #Guardamos las salidas\n",
        "model_short = Model(inputs = classifier.inputs, outputs = outputs) #Creamos nuevo modelo\n",
        "print(model_short.summary()) \n",
        "\n",
        "# disable the warnings\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "image_path = \"/content/RNC/test_set/cats\"\n",
        "\n",
        "images = []\n",
        "# make a prediction on the image\n",
        "images_data = []\n",
        "filenames = []\n",
        "\n",
        "# load all images into a list\n",
        "for img in os.listdir(image_path):\n",
        "        img = os.path.join(image_path, img)\n",
        "        img = image.load_img(img, target_size=(128,128))\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        # normalize the image\n",
        "        processed_image = np.array(img, dtype=\"float\") / 255.0\n",
        "        images.append(processed_image)\n",
        "        #Hacer predicciones\n",
        "        pred_result = model_short.predict(images)\n",
        "        images_data.append(pred_result)\n",
        "        filenames.append(img)\n",
        "        \n",
        "images = np.vstack(images)\n",
        "feat = pred_result[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_input (InputLayer)    [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 126, 126, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 61, 61, 64)        36928     \n",
            "=================================================================\n",
            "Total params: 38,720\n",
            "Trainable params: 38,720\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3e2a1e2dc537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#Hacer predicciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpred_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_short\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mimages_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer model expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 128, 128, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 128, 128, 3) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vO5XCNxJVpm"
      },
      "source": [
        "\n",
        "#Construir el modelo de CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPiSvJ1JVpq"
      },
      "source": [
        "#RED 2\n",
        "Inicializar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "303E6WJiJVps"
      },
      "source": [
        "red2 = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdosAOrXJVpt"
      },
      "source": [
        "# Paso 1 - Convolución y Maxpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cra3btt-JVpu"
      },
      "source": [
        "#Capa 1\n",
        "red2.add(Conv2D(filters = 32,kernel_size = (3, 3),input_shape = (128,128, 3), activation = \"relu\"))\n",
        "red2.add(MaxPooling2D(pool_size = (2,2)))\n",
        "#Capa 2\n",
        "red2.add(Conv2D(filters = 64,kernel_size = (3, 3), activation = \"relu\"))\n",
        "red2.add(MaxPooling2D(pool_size = (2,2)))\n",
        "#Capa 3\n",
        "red2.add(Conv2D(filters = 128,kernel_size = (3, 3),activation = \"relu\"))\n",
        "red2.add(MaxPooling2D(pool_size = (2,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfoFwwzzJVpv"
      },
      "source": [
        "# Paso 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0vV_vdNJVpw"
      },
      "source": [
        "red2.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-aDG3lcJVpx"
      },
      "source": [
        "# Paso 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_s9L7dlJVpy"
      },
      "source": [
        "#Capa 1\n",
        "red2.add(Dense(units = 128, activation = \"relu\"))\n",
        "red2.add(Dropout(0.5))\n",
        "\n",
        "#Capa 2\n",
        "red2.add(Dense(units = 128, activation = \"relu\"))\n",
        "red2.add(Dropout(0.5))\n",
        "\n",
        "#Salida\n",
        "red2.add(Dense(units = 1, activation = \"sigmoid\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaAX_H8FJVpz"
      },
      "source": [
        "# Compilar la CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nBEVKo-JVp0"
      },
      "source": [
        "red2.compile(optimizer = 'adam', loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "batch = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_44NWR1YJVp1"
      },
      "source": [
        "#Filtros capa 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vNFXFsZJVp2"
      },
      "source": [
        "#Mostrar los filtros\n",
        "layer2 = classifier.layers #Obtenemos las capas \n",
        "filters2,biases = classifier.layers[2].get_weights()\n",
        "print(layer[2].name,filters2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8S2xsxoJVp5"
      },
      "source": [
        "#Mostrar filtros\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DCSQ8PRJVp6"
      },
      "source": [
        "#Creamos la imagen para mostrar\n",
        "fig1 = plt.figure(figsize=(8,12))\n",
        "columnas = 8\n",
        "filas = 8\n",
        "nfiltros = columnas*filas\n",
        "\n",
        "#For para graficar todos los filtros\n",
        "for i in range(1,nfiltros+1):\n",
        "    f = filters2[:,:,:,i-1]\n",
        "    fig1 = plt.subplot(filas,columnas,i)\n",
        "    fig1.set_xticks([])\n",
        "    fig1.set_yticks([])\n",
        "    plt.imshow(f[:,:,0], cmap = 'gray')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5URpb5JJVp8"
      },
      "source": [
        "# Parte 2 - Ajustar la CNN a las imágenes para entrenar "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOm2nrmDJVp8"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_dataset = train_datagen.flow_from_directory('/content/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
        "                                                    target_size=(128, 128),\n",
        "                                                    batch_size=batch,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "testing_dataset = test_datagen.flow_from_directory('/content/deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
        "                                                target_size=(128, 128),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLmozEyPJVp-"
      },
      "source": [
        "#Entrenar la red"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnV1aAfbJVp-"
      },
      "source": [
        "history = red2.fit(training_dataset,\n",
        "                        steps_per_epoch=8000/32,\n",
        "                        epochs=25,\n",
        "                        validation_data=testing_dataset,\n",
        "                        validation_steps=2000/32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfJpMlI-JVp_"
      },
      "source": [
        "#Guardar el modelo \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykJ6trSLJVqA"
      },
      "source": [
        "#Cargar los pesos de las neuronas\n",
        "classifier.load_weights('best_weights.hdf5')\n",
        "#Guardar modelo\n",
        "classifier.save('RED2.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUgOI_uZJVqB"
      },
      "source": [
        "\"Mostrar los salidas red 2________________________________________________________________\"\n",
        "#Mostrar las imagenes de salida de las convoluciones\n",
        "conv_layer2 = [0,2] #indices de las capas convolucionales\n",
        "outputs2 = [red2.layers[i].output for i in conv_layer2] #Guardamos las salidas\n",
        "print(outputs2)\n",
        "model_short2 = Model(inputs = red2.inputs, outputs = outputs2) #Creamos nuevo modelo\n",
        "print(model_short2.summary()) \n",
        "\n",
        "# disable the warnings\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "image_path2 = \"/test_set/cats\"\n",
        "\n",
        "images2 = []\n",
        "    \n",
        "# load all images into a list\n",
        "for img2 in os.listdir(image_path2):\n",
        "        img2 = os.path.join(image_path2, img2)\n",
        "        img2 = image.load_img(img2, target_size=(128,128))\n",
        "        img2 = image.img_to_array(img2)\n",
        "        img2 = np.expand_dims(img2, axis=0)\n",
        "        # normalize the image\n",
        "        processed_image = np.array(img2, dtype=\"float\") / 255.0\n",
        "        images2.append(processed_image)\n",
        "        \n",
        "images2 = np.vstack(images2)\n",
        "\n",
        "# make a prediction on the image\n",
        "images_data2 = []\n",
        "filenames2 = []\n",
        "\n",
        "\n",
        "for filename2 in os.listdir(image_path2):    \n",
        "    pred_result2 = model_short2.predict(images2)\n",
        "    images_data2.append(pred_result2)\n",
        "    filenames2.append(filename2)\n",
        "\n",
        "\n",
        "feat2 = pred_result2[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqYQf00nryGS"
      },
      "source": [
        "# Sobreentrenada\n",
        "Predicción del 83.9 en testing y 88.4 en training,\n",
        "función de perdidas 0.4336 en testing y 0.2768 en training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVCxvY_RqbGM"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-7L3xRarTzY"
      },
      "source": [
        "#Perdidas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7oqcJ5Kqg2L"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJZKdbT1rYSO"
      },
      "source": [
        "#Presición\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w43ESzWam0ma"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}